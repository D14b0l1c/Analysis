#!/usr/bin/env python3
"""
Vulnerability Database Merger
Combines NVD, ExploitDB, and RouterSploit data into a unified vulnerability index
for vulnerability assessment.
"""

import pandas as pd
import os
import re
from datetime import datetime
import sqlite3
import json


class VulnerabilityMerger:
    def __init__(self):
        self.nvd_file = os.path.join("exploit_db", "nvd", "flattened_nvd.csv")
        self.exploitdb_file = os.path.join("exploit_db", "database_only", "safe_vulnerability_index.csv")
        self.routersploit_file = os.path.join("exploit_db", "routersploit", "routersploit_vulnerabilities.csv")
        self.output_dir = os.path.join("exploit_db", "index")
        
    def load_databases(self):
        """Load all vulnerability databases"""
        
        databases = {}
        
        # Load NVD data
        if os.path.exists(self.nvd_file):
            print("[*] Loading NVD database...")
            databases['nvd'] = pd.read_csv(self.nvd_file)
            print(f"[✓] Loaded {len(databases['nvd'])} NVD entries")
        else:
            print("[!] NVD database not found")
            databases['nvd'] = pd.DataFrame()
        
        # Load Safe ExploitDB data  
        if os.path.exists(self.exploitdb_file):
            print("[*] Loading safe ExploitDB vulnerability database...")
            databases['exploitdb'] = pd.read_csv(self.exploitdb_file)
            # Normalize column names
            databases['exploitdb'].columns = [col.strip().lower() for col in databases['exploitdb'].columns]
            print(f"[✓] Loaded {len(databases['exploitdb'])} safe vulnerability entries (CVE-referenced only)")
        else:
            print("[!] Safe ExploitDB database not found - run exploitdb_updater.py first")
            databases['exploitdb'] = pd.DataFrame()
            
        # Load RouterSploit data
        if os.path.exists(self.routersploit_file):
            print("[*] Loading RouterSploit database...")
            databases['routersploit'] = pd.read_csv(self.routersploit_file)
            print(f"[✓] Loaded {len(databases['routersploit'])} RouterSploit entries")
        else:
            print("[!] RouterSploit database not found")
            databases['routersploit'] = pd.DataFrame()
            
        return databases
    
    def extract_cves_from_exploitdb(self, df_exploitdb):
        """Process safe ExploitDB vulnerability data (already has CVEs extracted)"""
        
        print("[*] Processing safe ExploitDB vulnerability data...")
        
        if df_exploitdb.empty:
            print("[!] No ExploitDB data to process")
            return pd.DataFrame()
        
        # The safe vulnerability index already has CVEs extracted as 'vulnerability_ids'
        if 'vulnerability_ids' in df_exploitdb.columns:
            # Use existing CVE extraction
            df_exploitdb['extracted_cves'] = df_exploitdb['vulnerability_ids']
            print("[✓] Using pre-extracted CVE references from safe database")
        else:
            # Fallback to manual extraction if needed
            print("[*] Extracting CVEs from description field...")
            cve_pattern = re.compile(r'CVE-\d{4}-\d{4,7}', re.IGNORECASE)
            
            def extract_cves_from_description(description):
                if isinstance(description, str):
                    found_cves = cve_pattern.findall(description)
                    return ';'.join([cve.upper() for cve in found_cves])
                return ''
            
            df_exploitdb['extracted_cves'] = df_exploitdb['description'].apply(extract_cves_from_description)
        
        # Filter to only entries with CVEs (should already be filtered in safe database)
        df_with_cves = df_exploitdb[df_exploitdb['extracted_cves'] != ''].copy()
        
        print(f"[✓] Processed {len(df_with_cves)} safe vulnerability entries with CVE references")
        
        return df_with_cves
    
    def create_unified_index(self, databases):
        """Create unified vulnerability index"""
        
        print("[*] Creating unified vulnerability index...")
        
        unified_records = []
        
        # Process NVD entries
        if not databases['nvd'].empty:
            for _, row in databases['nvd'].iterrows():
                record = {
                    'cve_id': row.get('cve_id', ''),
                    'description': row.get('description', ''),
                    'cvss_score': row.get('cvss_v3_score', ''),
                    'cvss_severity': row.get('cvss_v3_severity', ''),
                    'vendors': row.get('vendors', ''),
                    'products': row.get('products', ''),
                    'cwe_ids': row.get('cwe_ids', ''),
                    'published_date': row.get('published', ''),
                    'modified_date': row.get('modified', ''),
                    'source': 'NVD',
                    'exploit_available': 'No',
                    'exploit_title': '',
                    'exploit_platform': '',
                    'exploit_type': '',
                    'routersploit_module': '',
                    'target_device': '',
                    'vulnerability_category': 'general'
                }
                unified_records.append(record)
        
        # Process ExploitDB entries with CVEs
        df_exploitdb_cves = self.extract_cves_from_exploitdb(databases['exploitdb'])
        
        for _, row in df_exploitdb_cves.iterrows():
            cves = row['extracted_cves'].split(';') if row['extracted_cves'] else []
            
            for cve in cves:
                if cve:
                    record = {
                        'cve_id': cve,
                        'description': row.get('description', ''),
                        'cvss_score': '',  # ExploitDB doesn't have CVSS
                        'cvss_severity': '',
                        'vendors': '',
                        'products': '',
                        'cwe_ids': '',
                        'published_date': row.get('date', ''),
                        'modified_date': '',
                        'source': 'ExploitDB',
                        'exploit_available': 'Yes',
                        'exploit_title': row.get('description', ''),
                        'exploit_platform': row.get('platform', ''),
                        'exploit_type': row.get('type', ''),
                        'routersploit_module': '',
                        'target_device': '',
                        'vulnerability_category': 'exploit_available'
                    }
                    unified_records.append(record)
        
        # Process RouterSploit entries
        if not databases['routersploit'].empty:
            for _, row in databases['routersploit'].iterrows():
                cves = row.get('cve_references', '').split(';') if row.get('cve_references') else []
                
                if cves and cves[0]:  # If CVE references exist
                    for cve in cves:
                        if cve:
                            record = {
                                'cve_id': cve,
                                'description': row.get('description', ''),
                                'cvss_score': '',
                                'cvss_severity': row.get('severity', ''),
                                'vendors': row.get('vendor', ''),
                                'products': row.get('device', ''),
                                'cwe_ids': '',
                                'published_date': row.get('date_added', ''),
                                'modified_date': '',
                                'source': 'RouterSploit',
                                'exploit_available': 'Yes',
                                'exploit_title': row.get('module_name', ''),
                                'exploit_platform': 'hardware',
                                'exploit_type': row.get('vulnerability_type', ''),
                                'routersploit_module': row.get('module_name', ''),
                                'target_device': f"{row.get('vendor', '')} {row.get('device', '')}".strip(),
                                'vulnerability_category': row.get('category', 'router_iot')
                            }
                            unified_records.append(record)
                else:
                    # RouterSploit entry without CVE - create synthetic entry
                    synthetic_id = f"RS-{row.get('vendor', 'UNK')}-{row.get('device', 'UNK')}-{hash(row.get('module_name', '')) % 10000}"
                    record = {
                        'cve_id': synthetic_id,
                        'description': row.get('description', ''),
                        'cvss_score': '',
                        'cvss_severity': row.get('severity', ''),
                        'vendors': row.get('vendor', ''),
                        'products': row.get('device', ''),
                        'cwe_ids': '',
                        'published_date': row.get('date_added', ''),
                        'modified_date': '',
                        'source': 'RouterSploit',
                        'exploit_available': 'Yes',
                        'exploit_title': row.get('module_name', ''),
                        'exploit_platform': 'hardware',
                        'exploit_type': row.get('vulnerability_type', ''),
                        'routersploit_module': row.get('module_name', ''),
                        'target_device': f"{row.get('vendor', '')} {row.get('device', '')}".strip(),
                        'vulnerability_category': row.get('category', 'router_iot')
                    }
                    unified_records.append(record)
        
        print(f"[✓] Created {len(unified_records)} unified vulnerability records")
        
        return pd.DataFrame(unified_records)
    
    def merge_duplicate_cves(self, df_unified):
        """Merge records with the same CVE ID"""
        
        print("[*] Merging duplicate CVE entries...")
        
        # Fill NaN values with empty strings
        df_unified = df_unified.fillna('')
        
        # Group by CVE ID and merge information
        merged_records = []
        
        def safe_join(series, separator=';'):
            """Safely join series values, handling NaN and empty strings"""
            filtered = series[series != ''].dropna().unique()
            return separator.join(str(x) for x in filtered if x)
        
        for cve_id, group in df_unified.groupby('cve_id'):
            if len(group) == 1:
                # Single record, keep as is
                merged_records.append(group.iloc[0].to_dict())
            else:
                # Multiple records for same CVE, merge them
                merged_record = {
                    'cve_id': cve_id,
                    'description': group[group['description'] != '']['description'].iloc[0] if len(group[group['description'] != '']) > 0 else '',
                    'cvss_score': group[group['cvss_score'] != '']['cvss_score'].iloc[0] if len(group[group['cvss_score'] != '']) > 0 else '',
                    'cvss_severity': group[group['cvss_severity'] != '']['cvss_severity'].iloc[0] if len(group[group['cvss_severity'] != '']) > 0 else '',
                    'vendors': safe_join(group['vendors']),
                    'products': safe_join(group['products']),
                    'cwe_ids': safe_join(group['cwe_ids']),
                    'published_date': group[group['published_date'] != '']['published_date'].iloc[0] if len(group[group['published_date'] != '']) > 0 else '',
                    'modified_date': group[group['modified_date'] != '']['modified_date'].iloc[0] if len(group[group['modified_date'] != '']) > 0 else '',
                    'source': safe_join(group['source']),
                    'exploit_available': 'Yes' if 'Yes' in group['exploit_available'].values else 'No',
                    'exploit_title': safe_join(group['exploit_title']),
                    'exploit_platform': safe_join(group['exploit_platform']),
                    'exploit_type': safe_join(group['exploit_type']),
                    'routersploit_module': safe_join(group['routersploit_module']),
                    'target_device': safe_join(group['target_device']),
                    'vulnerability_category': safe_join(group['vulnerability_category'])
                }
                merged_records.append(merged_record)
        
        print(f"[✓] Merged to {len(merged_records)} unique vulnerability records")
        
        return pd.DataFrame(merged_records)
    
    def create_sqlite_index(self, df_merged):
        """Create SQLite database for fast lookups"""
        
        print("[*] Creating SQLite vulnerability index...")
        
        db_file = os.path.join(self.output_dir, "vulnerability_index.db")
        
        # Remove existing database
        if os.path.exists(db_file):
            os.remove(db_file)
        
        # Create new database
        conn = sqlite3.connect(db_file)
        
        # Create vulnerabilities table
        df_merged.to_sql('vulnerabilities', conn, if_exists='replace', index=False)
        
        # Create indexes for fast lookup
        cursor = conn.cursor()
        
        # Index on CVE ID
        cursor.execute("CREATE INDEX idx_cve_id ON vulnerabilities(cve_id)")
        
        # Index on vendors
        cursor.execute("CREATE INDEX idx_vendors ON vulnerabilities(vendors)")
        
        # Index on products
        cursor.execute("CREATE INDEX idx_products ON vulnerabilities(products)")
        
        # Index on exploit availability
        cursor.execute("CREATE INDEX idx_exploit_available ON vulnerabilities(exploit_available)")
        
        # Index on severity
        cursor.execute("CREATE INDEX idx_severity ON vulnerabilities(cvss_severity)")
        
        # Full-text search index on description
        cursor.execute("""
            CREATE VIRTUAL TABLE vulnerabilities_fts USING fts5(
                cve_id, description, vendors, products, exploit_title,
                content='vulnerabilities'
            )
        """)
        
        cursor.execute("INSERT INTO vulnerabilities_fts SELECT cve_id, description, vendors, products, exploit_title FROM vulnerabilities")
        
        conn.commit()
        conn.close()
        
        print(f"[✓] SQLite index created: {db_file}")
    
    def save_merged_data(self, df_merged):
        """Save merged vulnerability data"""
        
        # Save to CSV (no timestamp)
        csv_file = os.path.join(self.output_dir, "vulnerability_index.csv")
        df.to_csv(csv_file, index=False)
        print(f"[✓] Index saved to: {csv_file}")
        
        # Create SQLite index
        self.create_sqlite_index(df_merged)
        
        # Generate statistics
        self.generate_statistics(df_merged)
        
        return df_merged
    
    def generate_statistics(self, df_merged):
        """Generate comprehensive statistics"""
        
        print(f"\n[*] Vulnerability Database Statistics")
        print("=" * 50)
        
        print(f"Total vulnerabilities: {len(df_merged)}")
        
        # By source
        source_counts = df_merged['source'].value_counts()
        print(f"\nBy source:")
        for source, count in source_counts.items():
            print(f"  - {source}: {count}")
        
        # Exploits available
        exploit_counts = df_merged['exploit_available'].value_counts()
        print(f"\nExploit availability:")
        for status, count in exploit_counts.items():
            print(f"  - {status}: {count}")
        
        # By severity (for entries that have it)
        severity_data = df_merged[df_merged['cvss_severity'] != '']
        if len(severity_data) > 0:
            severity_counts = severity_data['cvss_severity'].value_counts()
            print(f"\nBy CVSS severity ({len(severity_data)} entries):")
            for severity, count in severity_counts.items():
                print(f"  - {severity}: {count}")
        
        # By vulnerability category
        category_counts = df_merged['vulnerability_category'].value_counts()
        print(f"\nBy category:")
        for category, count in category_counts.items():
            print(f"  - {category}: {count}")
        
        # RouterSploit specific
        routersploit_data = df_merged[df_merged['source'].str.contains('RouterSploit', na=False)]
        if len(routersploit_data) > 0:
            print(f"\nRouterSploit coverage: {len(routersploit_data)} vulnerabilities")
            
            # Top vendors in RouterSploit
            rs_vendors = []
            for vendors_str in routersploit_data['vendors']:
                if vendors_str:
                    rs_vendors.extend(vendors_str.split(';'))
            
            if rs_vendors:
                vendor_counts = pd.Series(rs_vendors).value_counts().head(10)
                print(f"  Top RouterSploit vendors:")
                for vendor, count in vendor_counts.items():
                    print(f"    * {vendor}: {count}")
        
        # Recent vulnerabilities (if we have dates)
        dated_data = df_merged[df_merged['published_date'] != '']
        if len(dated_data) > 0:
            print(f"\nVulnerabilities with dates: {len(dated_data)}")
        
        print(f"\n[✓] Statistics generated successfully!")
    
    def merge_vulnerability_databases(self):
        """Main method to merge all vulnerability databases"""
        
        print("[*] Starting Vulnerability Database Merger")
        print("=" * 55)
        
        # Create output directory
        os.makedirs(self.output_dir, exist_ok=True)
        
        # Load all databases
        databases = self.load_databases()
        
        # Create unified index
        df_unified = self.create_unified_index(databases)
        
        # Merge duplicate CVEs
        df_merged = self.merge_duplicate_cves(df_unified)
        
        # Save merged data and create indexes
        final_df = self.save_merged_data(df_merged)
        
        print(f"\n[✓] Vulnerability database merger completed!")
        print(f"[✓] Ready for vulnerability assessment pipeline!")
        
        return final_df


if __name__ == "__main__":
    merger = VulnerabilityMerger()
    merger.merge_vulnerability_databases()